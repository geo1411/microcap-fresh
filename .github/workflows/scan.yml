name: scan-and-publish

on:
  schedule:
    - cron: "*/30 * * * *"   # every 30 minutes (UTC)
  workflow_dispatch: {}       # manual Run button
  push:
    branches: [ "main" ]      # re-deploy on any change to main

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scanner (wide, ranked, SL/TP/Fib)
        run: |
          mkdir -p out_upload2
          python newcoin_hunter.py \
            --out out_upload2 \
            --target_disc 350 --target_ref 200 \
            --limit_per_chain 3000 \
            --max_age_h_refine 1200 \
            --min_traders_refine 2 \
            --min_vliq_refine 0.08 \
            --max_fdv_refine 30000000 \
            --min_lp_refine 3000 \
            --max_lp_refine 2000000 \
            --require_social_refine false

      - name: Prepare site folder
        run: |
          rm -rf site && mkdir -p site
          # homepage = discovery; refined = separate page
          if [ -f out_upload2/candidates_discovery.html ]; then
            cp out_upload2/candidates_discovery.html site/index.html
          else
            echo "<!doctype html><meta charset='utf-8'><h1>No discovery results</h1>" > site/index.html
          fi
          [ -f out_upload2/candidates_refined.html ] && cp out_upload2/candidates_refined.html site/candidates_refined.html || true
          # CSVs
          for f in candidates_refined.csv candidates_discovery.csv rejects_refined.csv rejects_discovery.csv; do
            [ -f "out_upload2/$f" ] && cp "out_upload2/$f" site/ || true
          done

      - name: Stamp pages with build time (safe, no heredocs)
        run: |
          ts="$(date -u '+%Y-%m-%d %H:%M:%S')"
          stamp="<div style='padding:8px 12px;margin:12px 0;background:#fff9d6;border:1px solid #eee;font:14px system-ui'>Last updated: ${ts} UTC • Run ID: ${GITHUB_RUN_ID} • Commit: ${GITHUB_SHA}</div>"
          for f in site/index.html site/candidates_refined.html; do
            [ -f "$f" ] || continue
            awk -v STAMP="$stamp" '
              BEGIN{added=0}
              /<body[^>]*>/{print; print STAMP; added=1; next}
              {print}
              END{if(!added){print STAMP}}
            ' "$f" > "$f.tmp" && mv "$f.tmp" "$f"
            # add a cache-busting meta tag too
            awk -v VER="${GITHUB_RUN_ID}" '
              BEGIN{done=0}
              /<head[^>]*>/{print; print "<meta http-equiv=\"Cache-Control\" content=\"no-cache, no-store, must-revalidate\">"; print "<meta name=\"x-run\" content=\"" VER "\">"; done=1; next}
              {print}
            ' "$f" > "$f.tmp" && mv "$f.tmp" "$f"
          done

      - name: Upload artifact (for Pages)
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    needs: build
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
